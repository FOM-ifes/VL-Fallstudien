---
title: "Lösungsskizze: Hauptkomponentenanalyse (HKA)"
lang: de
author: "Arbeitsgruppe quantitative Methodenausbildung am ifes"
date: last-modified
format: 
  html:
    toc: true
    html-math-method: katex
  pdf:
    toc: true
    number-sections: false
    colorlinks: true 
    papersize: a4
---

```{r}
#| label: setup
#| include: false

library(mosaic)
library(psych)
library(corrplot)
library(REdaS)
library(here)
```

# Grundidee

Die Hauptkomponentenanalyse (HKA, engl. Principal Component Analysis, PCA) ist eine statistische Methode, mit der sich eine Vielzahl von Variablen (Spalten) zu wenigen, möglichst aussagekräftigen Hauptkomponenten zusammenfassen lassen. 
Das Ziel dieser Hauptkomponenten ist, Datentabellen übersichtlicher zu machen (Dimensionsreduktion) und dabei gleichzeitig so wenige Informationen wie möglich zu verlieren.

Die entstandenen Hauptkomponenten sind voneinander unabhängig und können als *normale* Variablen für weitere statistische Tests verwendet werden.

# Projekt IRLL (International Readiness in Lifelong Learning)

Selbstgesteuertes Lernen und überfachliche Kompetenzen gewinnen vor dem Hintergrund dynamischer und volatiler Arbeits- und Gesellschaftsprozesse zunehmend an Bedeutung. 
Um dem gerecht zu werden, bedarf es der Herausbildung und Stärkung einer fächerübergreifenden „Learning Readiness" im Studium. 
Außercurriculare Angebote stellen in diesem Zusammenhang wichtige Bausteine dar. 
Im Projekt IRLL (<https://forschung.fom.de/forschung/institute/iap-institut-fuer-arbeit-und-personal/irll.html/)> der FOM Hochschule wurde untersucht, inwieweit Hürden bei der Wahrnehmung studienübergreifender Angebote bestehen und wie entsprechenden Bedarfen von Hochschulseite begegnet werden kann.

Im Zeitraum von September bis November 2022 wurden Bachelor- und Masterstudierende sowie Promovierende aller Fachbereiche, Semester und Standorte der FOM Hochschule befragt.
Schwerpunkte der Befragung waren die Wahrnehmung und Entwicklung von Kompetenzen, die Motivation zum überfachlichen Kompetenzerwerb sowie die Wahrnehmung bestehender Auslands- und Zusatzprogramme. 
Insgesamt haben 1.421 Studierende teilgenommen. 
Die Anzahl der in der nachfolgenden Analyse berücksichtigten Fälle (nach Bereinigung) beträgt $n = 875$.

Über die für die nachfolgende Hauptkomponentenanalyse verwendeten Variablen wurde abgefragt, inwieweit die befragten Personen a) dem eigenen Empfinden nach über verschiedene studienübergreifende Kompetenzen verfügen (A020_A021 - A020_A030) und b) in welchem Maße sie einen Ausbau dieser Kompetenzen als wichtig erachten (A040_A041 - A040_A050). 
Über den Fragebogen sowie die Legende zur Datentabelle können die Fragestellungen sowie die Bezeichnungen für die einzelnen Variablen nachgelesen werden.

## Daten einlesen und inspizieren

<!-- 
Technischer Hinweis: 
Das Paket "here" (https://here.r-lib.org/) ermöglicht einen einfacheren Umgang mit Datei(pfaden) innerhalb von RStudio Projekten. 
Die csv Datei "irll.csv" befindet sich im Projektordner "data". 
-->

```{r}
#| label: data

# Daten einlesen
irll <- here("data", "irll.csv") |> read.csv()

# Datenstruktur
str(irll)
```

#### Fragen

- Was ist eine Beobachtungseinheit in der Datentabelle?

  *Antwort:* Eine befragte Person.

- Wie wurden die Variablen erhoben und welches Skalenniveau besitzen die Variablen?

  *Antwort:* Die Variablen wurden mit einer 5-Punkt-Likert-Skala erhoben. Das Skalenniveau der Variablen ist quasi-metrisch. *Hinweis:* Die einzelnen Items sind eigentlich ordinal, der Einfachheit halber wird auch auch von quasi-metrisch ausgegangen.

- Welche Variablen würden sich für eine Hauptkomponentenanalyse anbieten?

  *Antwort:* Alle Variablen der Datentabelle können mithilfe der HKA zusammengefasst werden.

## Daten studentisieren

Die Hauptkomponentenanalyse ist sensitiv bezüglich unterschiedlicher Varianzen. 
Das bedeutet, dass Merkmale mit einer sehr hohen Varianz dominieren. 
Aus diesem Grund wird die Bildung der Hauptkomponenten auf Basis von studentisierten Variablen durchgeführt.

```{r}
#| label: z-trans

# Studentisierung
irll_norm <- irll |> scale() |> as.data.frame()
```

#### Frage

- Warum löst die Studentisierung das Problem der Varianz-Sensitivität der HKA?

  *Antwort:* Die studentisierten Variablen haben alle eine Varianz von eins.

## Eignung der Daten

Die Bildung der einzelnen Hauptkomponenten geschieht durch das Zusammenfassen von hoch korrelierten Variablen. 
Eine hohe Korrelation zwischen zwei Variablen deutet darauf hin, dass diese ähnliche oder nahezu gleiche Informationen beinhalten (Redundanz). 
Die HKA benutzt diese Redundanz, um sich ähnelnde Variablen zu einer Hauptkomponente zusammenzufassen.

Die Korrelation kann deshalb Hinweise darüber geben, ob sich die Daten für eine Hauptkomponentenanalyse eignen. 
Einen guten Überblick über die Korrelation zwischen den Variablen verschafft ein Korrelationsplot:

```{r}
#| label: cor

# Korrelationsmatrix
cor_mtx <- cor(irll_norm)

# Korrelationsplot
corrplot(cor_mtx)
```

#### Fragen

- Ist es egal, ob wir uns die Korrelations- oder Kovarianz-Matrix anschauen? Begründen Sie Ihre Antwort.

  *Antwort:* In diesem Fall schon, da die Daten studentisiert wurden und $cor(Z)==cov(Z)$. Allgemein gilt dies aber nicht.

- Wie kann der blaue Punkt zwischen A020_A022 und A020_A021 interpretiert werden? 
Und wie kann der (blaß) rote Punkt zwischen A040_A050 und A020_A030 interpretiert werden?

  - *Antwort (blau):* Der lineare Zusammenhang zwischen der Selbsteinschätzung der eigenen Fähigkeiten in Bezug auf das Verständnis wissenschaftlicher Texte in englischer Sprache und der Selbsteinschätzung der eigenen Fähigkeiten in Bezug auf das Verständnis von Fachvorträgen in englischer Sprache ist stark positiv.
  - *Interpretation:* Je besser Teilnehmende wissenschaftliche Texte in englischer Sprache verstehen, desto besser können sie Fachvorträgen in englischer Sprache folgen.
  - *Antwort (rot):* Der lineare Zusammenhang zwischen der Selbsteinschätzung der eigenen Fähigkeiten in Bezug auf Stressmanagement und der Wichtigkeit der eigenen Kompetenz bei Stressmanagement ist leicht negativ.
  - *Interpretation:* Je kompetenter Teilnehmende sich im Bereich Stressmanagement einschätzen, desto weniger Bedarf sehen sie in einem zusätzlichen Ausbau dieser Kompetenz.

- Warum sind manche Flächen im Korrelationsplot weiß und was bedeutet das?

  *Antwort:* Eine Korrelation von null bedeutet, dass die beiden Variablen linear nicht voneinander abhängen.

- Welche Gruppen von Variablen, die sich ähnlich sind, können Sie entdecken?

  *Mögliche Antwort:* A020_A021 bis A020_A024, A020_A025 bis A020_A030, A050_A051 bis A050_A054, A050_A045 bis A050_A050, 

- Denken Sie, dass sich die Daten für eine Hauptkomponentenanalyse eignen?

  *Antwort:* individuell

### KMO-Kriterium

Zusätzlich zur Betrachtung des Korrelationsplot gibt es Kennzahlen, welche die Eignung der Daten für eine HKA überprüfen. 
Einer dieser Kennzahlen ist das Kaiser-Meyer-Olkin-Kriterium (Kaiser und Rice, 1974), welches aus den partiellen Korrelationen zwischen Variablenpaaren, also der Korrelationen jeder Variable mit jeder anderen Variablen, berechnet wird. 
Die möglichen Ergebnisse des KMO-Kriteriums variieren zwischen null und eins. 
Damit die Daten für eine HKA geeignet sind, sollte das Ergebnis einen Wert von 0.5 nicht unterschreiten.

```{r}
#| label: kmo

# Kaiser-Maier-Olkin-Kriterium
KMOS(irll_norm)$KMO
```

#### Frage

- Hat das KMO-Kriterium etwas an Ihrer Annahme über die Eignung der Daten geändert? Begründen Sie.

  *Antwort:* Individuell. Begründung: Die Daten sind für eine HKA geeignet sind, da 0.817 > 0.5.

### Bartlett-Test

Ein Test, um herauszufinden, ob sich die Daten für eine HKA eignen, ist der sog. Bartlett-Test. 
Dieser überprüft, ob sich die Korrelationsmatrix von einer Einheitsmatrix unterscheidet.
Wenn sich die Daten für eine HKA eignen, dann sollte der Bartlett-Test die Nullhypothese, die Korrelationsmatrix unterscheidet sich nicht von einer Einheitsmatrix, verwerfen.

```{r}
#| label: bartlett

# Bartlett-Test
bart_spher(irll_norm)
```

#### Frage

- Zu welchem Ergebnis kommt der Bartlett-Test?

  *Antwort:* Ja. Die Nullhypothese, dass sich die Korrelationsmatrix nicht von einer Einheitsmatrix unterscheidet, kann zu jeder gängigen Irrtumswahrscheinlichkeit verworfen werden.

## Anzahl der Hauptkomponenten

Da sich die Daten für eine Hauptkomponentenanalyse eignen, wird im Anschluss die Anzahl der Hauptkomponenten bestimmt, welche ausreichend sind, um die wesentliche Struktur der betrachteten Merkmale abzubilden. 
Dies geschieht unter anderem anhand der Eigenwerte[^eigen] unserer Datenmatrix.

[^eigen]: Wird die Matrix mit einem Vektor multipliziert, ergibt sich als Ergebnis wieder ein Vektor. 
Für quadratische Matrizen existieren bestimmte Vektoren -- sogenannte *Eigenvektoren* --, die, wenn sie mit der Matrix multipliziert werden, ein Vielfaches des ursprünglichen Vektors ergeben. 
D. h., der Eigenvektor ändert seine Richtung nicht, er wird lediglich skaliert. 
Den Skalierungsfaktor wird als *Eigenwert* einer Matrix bezeichnet.

### Screeplot

Eine Möglichkeit zur Bestimmung der optimalen Anzahl der Hauptkomponenten ist die grafische Darstellung der Eigenwerte, der sog. Screeplot. 
Dabei entspricht die optimale Anzahl der Hauptkomponenten der Anzahl der Eigenwerte, die vor dem Knick des Graphen (*Ellenbogen*) liegen. 
Bei mehreren Knickstellen ist der stärkere bzw. weiter rechts stehende Knick ausschlaggebend.

```{r}
#| label: screeplot

# Screeplot
scree(irll_norm, factors = FALSE)
```

#### Frage

- Wie viele Hauptkomponenten würden Sie auf Basis des Screeplots bestimmen? Begründen Sie Ihre Antwort.
  
  *Antwort:* Der erste deutliche Knick ist bei fünf, der zweite (etwas stärkere) bei sieben, daher wären nach diesem Kriterium sechs Hauptkomponten zu wählen.

Der Screeplot ist zwar eine sehr beliebte Methode zur Bestimmung der Hauptkomponenten, allerdings ist er wie im vorliegenden Beispiel nicht immer eindeutig. 
In solchen Fällen kann auf andere Methoden und Faustregeln zurückgegriffen werden.

### Eigenwert über eins (Kaiser-Dickman-Kriterium)

Ein weiteres Kriterium zur Bestimmung der optimalen Anzahl von Hauptkomponenten ist die Betrachtung der Eigenwerte. 
Berücksichtigt werden dabei nur die Hauptkomponenten, deren Eigenwert größer als eins ist.

#### Frage

- Wie viele Hauptkomponenten ergeben sich nach dem Kaiser-Dickmann-Kriterium?

  *Antwort:* Nach diesem Kriterium ergibt sich sechs als Anzahl der Hauptkomponenten (was mit der Auswahl gemäß Screeplot übereinstimmt).

### Varianz

Auch der Anteil an Varianz, welcher durch die Hauptkomponenten erklärt werden kann, dient als Maßstab zur Bestimmung der optimalen Anzahl. 
Demnach sollte durch die Hauptkomponenten ein möglichst großer Teil der ursprünglichen Variation erklärt werden.

Dies kann mithilfe der Eigenwerte der Korrelationsmatrix abgeschätzt werden. 
Die einzelnen Eigenwerte werden in Relation zu der Summe aller Eigenwerte gesetzt. 
Zusätzlich zum Anteil der Varianz der einzelnen Hauptkomponenten an der Gesamtvarianz wird die kumulierte Varianz ausgegeben.

```{r}
#| label: eigenwerte

# Eigenwerte 
eigenwerte <- eigen(cor_mtx)$values

# Varianzanteil der Hauptkomponenten
(eigenwerte/sum(eigenwerte)) |> round(3)

# Kumulierte Varianz der Hauptkomponenten
cumsum(eigenwerte/sum(eigenwerte)) |> round(3)
```

#### Fragen

- Wie viel Prozent der Gesamtvarianz wird durch die zweite Hauptkomponente erklärt?

  *Antwort:* ca. 17.7 % (Varianzanteil)

- Wie viel Prozent der Gesamtvarianz erklären die ersten 10 Hauptkomponenten?

  *Antwort:* ca. 84.3 % (kumulierte Varianz)

- Ändert das Varianz-Kriterium etwas an der Anzahl der ausgewählten Hauptkomponenten?

  *Antwort:* Nein. 71.8% ist ausreichend viel Varianz.

## Analyse

Im folgenden Abschnitt wird die Hauptkomponentenanalyse mithilfe von `principal()` durchgeführt. 
in der Ausgabe werden die Ladungen (Korrelationen der ursprünglichen Variablen mit den Hauptkomponenten) ab einem Betrag von 0.5 angezeigt, gerundet und sortiert (jeweils innerhalb einer Komponente nach absteigenden Beträgen der Ladungen[^ladung]). 
Im unteren Teil der Ausgabe werden die Eigenwerte (`SS (sum of squares) loadings`), der Anteil der Hauptkomponenten an der Gesamtvarianz (`Proportion Var`), sowie die kumulierte Varianz (`Cumulative Var`) angegeben.[^principal]

[^ladung]: Als Ladung wird die Korrelation der ursprünglichen Variablen mit der Hauptkomponente bezeichnet.
[^principal]: Um die Ausgabe etwas übersichtlicher zu gestalten, wird anschließend an die Analyse noch ein nicht benötigter Teil der Ausgabe gelöscht.

```{r}
#| label: hka

# Hauptkomponentenanalyse ohne Rotation
irll_hka <- principal(irll_norm, 6, rotate = "none")
# Löschen des nicht benötigten Outputs
irll_hka$criteria <- NULL
# Grenzwert, sortieren und runden
print(irll_hka, cut = 0.5, sort = TRUE, digits = 2)
```

Wenn Sie sich die Ladungen der einzelnen Variablen auf die Hauptkomponenten anschauen, so können Sie erkennen, dass einige Variablen auf mehrere Hauptkomponenten hoch laden. 
Dieser Umstand erschwert eine inhaltliche Interpretation der einzelnen Hauptkomponenten.

## Rotation

Um die Hauptkomponenten inhaltlich besser interpretieren zu können, wird eine Rotation der Hauptkomponenten vorgenommen. 
Das Ziel der Rotation ist, dass auf jeder Hauptkomponente einige Variablen hoch und die übrigen Variablen möglichst niedrig laden. 
Zusätzlich sollte jede Variable nur auf eine einzelne Hauptkomponente hoch laden, auf die übrigen niedrig.

Es gibt zwei verschiedene Arten von Rotationsverfahren, orthogonale und oblique Verfahren. 
Orthogonale Rotationsverfahren nehmen Unabhängigkeit zwischen den extrahierten Hauptkomponenten an und produzieren einfache Modelle, die sich gut interpretieren lassen.
Allerdings bilden orthogonale Verfahren durch die Annahme der Unabhängigkeit die Realität meistens nicht so gut ab, wie oblique Rotationsverfahren, welche wiederum etwas schwieriger zu interpretieren sind.

Eine oft verwendete orthogonale Rotationsmethode zur besseren Interpretation der Hauptkomponenten ist die Varimax-Rotation:

```{r}
#| label: hka rotiert

# Varimax Rotation
irll_hka_rotiert <- principal(irll_norm, 6, rotate = "varimax")
# Löschen des nicht benötigten Outputs
irll_hka_rotiert$criteria <- NULL
# Grenzwert, sortieren und runden
print(irll_hka_rotiert, cut = 0.5, sort = TRUE, digits = 2)
```

Die sechs Faktoren lassen sich inhaltlich wie folgt bezeichnen:

1.  A020_A021 - A020-A024: Internationale Kompetenz (Kompetenzempfinden)
2.  A040_A041 - A040-A044: Internationale Kompetenz (Relevanzempfinden Kompetenzausbau)
3.  A020_A025 - A020-A027: Wiss. Kompetenz (Kompetenzempfinden)
4.  A040_A045 - A040-A047: Wiss. Kompetenz (Relevanzempfinden Kompetenzausbau)
5.  A020_A028 - A020-A030: Organisations- und Selbstkompetenz (Kompetenzempfinden)
6.  A040_A048 - A040-A050: Organisations- und Selbstkompetenz (Relevanzempfinden Kompetenzausbau)

Das Ergebnis zeigt, dass die befragten Studierenden innerhalb dreier voneinander abgrenzbarer Kompetenzdimensionen (*Organisations- und Selbstkompetenz*, *Wissenschaftliche Kompetenz*, *Internationale Kompetenz*) über ein unterschiedliches Kompetenzempfinden sowie verschiedene Bedarfe eines Ausbaus verfügen. 
Zudem wird deutlich, dass Kompetenzempfinden und -bedarf voneinander losgelöst zu betrachten sind. 
Ein Bedarf an Kompetenzausbau kann unabhängig von einem bestehenden Kompetenzempfinden in derselben Dimension vorhanden sein.

Der `Fit based upon off diagonal values` ist zwischen der unrotierten und der rotierten HKA gleich geblieben (größere Werte sind besser, das Kriterium kann ggf. bei der Auswahl der Anzahl Hauptkomponenten herangezogen werden).
Die `Mean item complexity` hat allerdings beim rotierten Modell abgenommen, im Schnitt laden also weniger Variablen auf die Hauptkomponenten und damit wird die Interpretation einfacher.[^ausgabe]

[^ausgabe]: Die weiteren Elemente der Ausgabe werden z. B. in Gehrke (2022), S. 345 ff. erläutert.

## HKA mit nicht standardisierten Daten

Tatsächlich kann die Hauptkomponentenanalyse mit `principal()` (Paket `psych`) auch ohne vorherige Standardisierung durch geführt werden, da `principal()` die Korrelationsmatrix nutzt.

```{r}
#| label: hka nicht studentisiert

# Varimax Rotation
irll_nichtstd <- principal(irll, 6, rotate = "varimax")
# Löschen des nicht benötigten Outputs
irll_nichtstd$criteria <- NULL
# Grenzwert, sortieren und runden
print(irll_nichtstd, cut = 0.5, sort = TRUE, digits = 2)
```


# Your Turn

Jetzt sind Sie an der Reihe! 
Im folgenden Code-Anfang wird das Paket `HDclassif` aktiviert.[^HDcl]
Dieses Paket enthält die Datentabelle `wine`. 
Diese Datentabelle ist das Ergebnis einer chemischen Analyse von Weinen, die in der gleichen Region in Italien angebaut werden, aber von drei verschiedenen Weinsorten (Barolo, Grignolino, Barbera) stammen. 
Bei der Analyse wurden 13 Inhaltsstoffe bestimmt, die in jeder der drei Weinsorten enthalten sind.[^wine]

[^HDcl]: GGf. muss das Paket vorher installiert werden, dazu muss der entsprechende Befehl ausgeführt werden (Kommentarzeichen `#` entfernen, nach Installation wieder auskommentieren).

[^wine]: Die Variablennamen wurden ergänzt, hier die Erklärungen zu einigen:
*Malic acid* -- Apfelsäure;
*Ash* -- Mineralstoffe;
*Alcalinity of ash* -- Säureneutralisierung;
*Total phenols* -- Phenolgehalt, hat mit Bitterkeit und Astringenz zu tun;
*Flavanoids* -- natürliche Farbstoffe;
*Proanthocyanins* -- haben mit Farbe, Bitterkeit, Aroma und Alterungspotential zu tun;
*Hue* -- Farbton; 
*OD280/OD315 (of diluted wines)* -- Maß für Proteinreinheit, je höher, desto bitterer;
*Proline* -- Aminosäure, Süße.


```{r}
#| label: Wine

# Wine-Datentabelle laden
# install.packages("HDclassif") ## ggf. installieren
data(wine, package = "HDclassif")

# Klassifikation entfernen
wine <- wine[, -1]
# Variablen benennen
names(wine) <- c("Alcohol", "Malic acid", "Ash", "Alcalinity of ash", "Magnesium", "Total phenols", "Flavanoids", "Nonflavanoid phenols", "Proanthocyanins", "Color intensity", "Hue", "OD280/OD315", "Proline")


### Musterlösung: ###

# Daten studentisieren
wine_norm <- wine |> scale() |> as.data.frame()

# Korrelationsplot
cor_mtx <- cor(wine_norm)
# cor_mtx
corrplot(cor_mtx,tl.col="black", tl.cex=0.8)

# Eignung der Daten
KMOS(wine_norm)$KMO
bart_spher(wine_norm)

# Screenplot
scree(wine_norm, factors = FALSE)

# HKA unter Nutzung der Varimax-Rotation
hka_rotiert <- principal(wine_norm, 3, rotate = "varimax")
hka_rotiert$criteria <- NULL
print(hka_rotiert, cut = 0.5, sort = TRUE, digits = 2)
```

*Antwort:* 

Die Daten sind gemäß KMO-Kriterium und Bartlett-Test für eine HKA geeignet.

Der Screeplot hat den Knick bei vier Hauptkomponenten, daher sollten drei gewählt werden. 
Dies stimmt auch mit dem Eigenwertkriterium überein.

Die Hauptkomponenten erklären 67 % der Varianz.

